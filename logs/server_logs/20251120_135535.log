2025-11-20 13:55:35,958 - src.main - INFO - Logging initialized. Log file: logs/server_logs/20251120_135535.log
2025-11-20 13:55:35,969 - src.main - INFO - Frontend assets mounted at /assets
2025-11-20 13:55:35,970 - src.main - INFO - SPA routing configured for frontend
2025-11-20 13:55:36,235 - uvicorn.error - INFO - Started server process [10]
2025-11-20 13:55:36,238 - uvicorn.error - INFO - Waiting for application startup.
2025-11-20 13:55:36,240 - src.main - INFO - ðŸš€ Ultimate Advisor API server started successfully
2025-11-20 13:55:36,240 - src.main - INFO - ðŸ“š RAG-based chat system for Ultimate Frisbee rules and guidance
2025-11-20 13:55:36,241 - uvicorn.error - INFO - Application startup complete.
2025-11-20 13:55:36,250 - uvicorn.error - INFO - Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-11-20 13:55:38,229 - src.main - INFO - Request: GET / from 172.67.182.229
2025-11-20 13:55:38,237 - src.main - INFO - Response: 200 for GET / - 0.008s
2025-11-20 13:55:38,368 - src.main - INFO - Request: GET /assets/index-D0ZgT6O3.js from 172.67.182.229
2025-11-20 13:55:38,369 - src.main - INFO - Request: GET /assets/index-5otq0lqL.css from 172.67.182.229
2025-11-20 13:55:38,377 - src.main - INFO - Response: 304 for GET /assets/index-D0ZgT6O3.js - 0.010s
2025-11-20 13:55:38,381 - src.main - INFO - Response: 304 for GET /assets/index-5otq0lqL.css - 0.012s
2025-11-20 13:55:39,345 - src.main - INFO - Request: GET /history/queries from 172.67.182.229
2025-11-20 13:55:39,594 - src.main - INFO - Request: GET /piramal-favicone.png from 172.67.182.229
2025-11-20 13:55:39,595 - src.main - INFO - Response: 200 for GET /history/queries - 0.250s
2025-11-20 13:55:39,602 - src.main - INFO - Response: 200 for GET /piramal-favicone.png - 0.008s
2025-11-20 13:56:00,907 - src.main - INFO - Request: POST /rag/query from 172.67.182.229
2025-11-20 13:56:00,960 - src.rag.repositories - INFO - Models configured: LLM=gemma3:4b, Embedding=embeddinggemma:latest
2025-11-20 13:56:01,322 - httpx - INFO - HTTP Request: POST http://ollama:11434/api/embed "HTTP/1.1 200 OK"
2025-11-20 13:56:01,325 - src.rag.repositories - INFO - Embedding dimension confirmed: 768
2025-11-20 13:56:01,353 - src.rag.repositories - INFO - Database connection established
2025-11-20 13:56:01,355 - src.rag.repositories - INFO - pgvector extension ensured
2025-11-20 13:56:01,359 - src.rag.repositories - INFO - Vector store configured with table 'documents' (embed_dim=768)
2025-11-20 13:56:01,417 - src.rag.repositories - ERROR - No documents in vector store - cannot perform queries
2025-11-20 13:56:01,418 - src.rag.repositories - ERROR - Failed to execute query: No documents in vector store
2025-11-20 13:56:01,419 - src.rag.repositories - ERROR - Traceback: Traceback (most recent call last):
  File "/app/src/rag/repositories.py", line 195, in query
    raise ValueError("No documents in vector store")
ValueError: No documents in vector store

2025-11-20 13:56:01,419 - src.rag.services - ERROR - Query failed for 'query='hi' top_k=2': No documents in vector store
2025-11-20 13:56:01,482 - src.history.repositories - INFO - Created query history with ID: 431f9ae4-2754-4f06-a478-364b2ec2138d
2025-11-20 13:56:01,485 - src.history.services - INFO - Saved query history with ID: 431f9ae4-2754-4f06-a478-364b2ec2138d
2025-11-20 13:56:01,485 - src.main - INFO - Response: 200 for POST /rag/query - 0.578s
2025-11-20 13:56:01,496 - src.main - INFO - Request: GET /history/queries from 172.67.182.229
2025-11-20 13:56:01,564 - src.main - INFO - Response: 200 for GET /history/queries - 0.069s
2025-11-20 13:57:54,971 - src.main - INFO - Request: POST /rag/query from 172.67.182.229
2025-11-20 13:57:55,060 - src.rag.repositories - INFO - Models configured: LLM=gemma3:4b, Embedding=embeddinggemma:latest
2025-11-20 13:57:56,119 - httpx - INFO - HTTP Request: POST http://ollama:11434/api/embed "HTTP/1.1 200 OK"
2025-11-20 13:57:56,120 - src.rag.repositories - INFO - Embedding dimension confirmed: 768
2025-11-20 13:57:56,146 - src.rag.repositories - INFO - Database connection established
2025-11-20 13:57:56,150 - src.rag.repositories - INFO - pgvector extension ensured
2025-11-20 13:57:56,158 - src.rag.repositories - INFO - Vector store configured with table 'documents' (embed_dim=768)
2025-11-20 13:57:56,174 - src.rag.repositories - ERROR - No documents in vector store - cannot perform queries
2025-11-20 13:57:56,175 - src.rag.repositories - ERROR - Failed to execute query: No documents in vector store
2025-11-20 13:57:56,176 - src.rag.repositories - ERROR - Traceback: Traceback (most recent call last):
  File "/app/src/rag/repositories.py", line 195, in query
    raise ValueError("No documents in vector store")
ValueError: No documents in vector store

2025-11-20 13:57:56,177 - src.rag.services - ERROR - Query failed for 'query='your model?' top_k=2': No documents in vector store
2025-11-20 13:57:56,230 - src.history.repositories - INFO - Created query history with ID: cadca30c-93e6-423f-9551-874740c5db98
2025-11-20 13:57:56,256 - src.history.services - INFO - Saved query history with ID: cadca30c-93e6-423f-9551-874740c5db98
2025-11-20 13:57:56,277 - src.main - INFO - Response: 200 for POST /rag/query - 1.306s
2025-11-20 13:57:56,315 - src.main - INFO - Request: GET /history/queries from 172.67.182.229
2025-11-20 13:57:56,418 - src.main - INFO - Response: 200 for GET /history/queries - 0.102s
2025-11-20 13:58:19,464 - src.main - INFO - Request: GET /history/queries/cadca30c-93e6-423f-9551-874740c5db98 from 172.67.182.229
2025-11-20 13:58:19,513 - src.main - INFO - Response: 200 for GET /history/queries/cadca30c-93e6-423f-9551-874740c5db98 - 0.049s
2025-11-20 13:58:20,825 - src.main - INFO - Request: GET /history/queries/431f9ae4-2754-4f06-a478-364b2ec2138d from 172.67.182.229
2025-11-20 13:58:20,879 - src.main - INFO - Response: 200 for GET /history/queries/431f9ae4-2754-4f06-a478-364b2ec2138d - 0.053s
2025-11-20 14:10:09,101 - src.main - INFO - Request: GET / from 172.67.182.229
2025-11-20 14:10:09,135 - src.main - INFO - Response: 200 for GET / - 0.039s
2025-11-20 14:10:09,229 - src.main - INFO - Request: GET /assets/index-D0ZgT6O3.js from 172.67.182.229
2025-11-20 14:10:09,239 - src.main - INFO - Request: GET /assets/index-5otq0lqL.css from 172.67.182.229
2025-11-20 14:10:09,240 - src.main - INFO - Response: 304 for GET /assets/index-D0ZgT6O3.js - 0.011s
2025-11-20 14:10:09,252 - src.main - INFO - Response: 304 for GET /assets/index-5otq0lqL.css - 0.013s
2025-11-20 14:10:10,528 - src.main - INFO - Request: GET /history/queries from 172.67.182.229
2025-11-20 14:10:10,592 - src.main - INFO - Request: GET /piramal-favicone.png from 172.67.182.229
2025-11-20 14:10:10,797 - src.main - INFO - Response: 200 for GET /history/queries - 0.268s
2025-11-20 14:10:10,802 - src.main - INFO - Response: 200 for GET /piramal-favicone.png - 0.210s
2025-11-20 14:10:16,848 - src.main - INFO - Request: POST /rag/query from 172.67.182.229
2025-11-20 14:10:16,985 - src.rag.repositories - INFO - Models configured: LLM=gemma3:4b, Embedding=embeddinggemma:latest
2025-11-20 14:10:17,250 - httpx - INFO - HTTP Request: POST http://ollama:11434/api/embed "HTTP/1.1 200 OK"
2025-11-20 14:10:17,254 - src.rag.repositories - INFO - Embedding dimension confirmed: 768
2025-11-20 14:10:17,279 - src.rag.repositories - INFO - Database connection established
2025-11-20 14:10:17,281 - src.rag.repositories - INFO - pgvector extension ensured
2025-11-20 14:10:17,299 - src.rag.repositories - INFO - Vector store configured with table 'documents' (embed_dim=768)
2025-11-20 14:10:17,310 - src.rag.repositories - INFO - Vector store contains 793 documents
2025-11-20 14:10:17,311 - src.rag.repositories - INFO - Index not initialized, creating from vector store...
2025-11-20 14:10:17,958 - src.rag.repositories - INFO - âœ“ Index successfully created from vector store
2025-11-20 14:10:17,959 - src.rag.repositories - INFO - Executing query: 'hi...'
2025-11-20 14:10:18,932 - httpx - INFO - HTTP Request: POST http://ollama:11434/api/show "HTTP/1.1 200 OK"
2025-11-20 14:10:19,183 - httpx - INFO - HTTP Request: POST http://ollama:11434/api/embed "HTTP/1.1 200 OK"
2025-11-20 14:12:19,800 - src.rag.repositories - ERROR - Failed to execute query: timed out
2025-11-20 14:12:20,374 - src.rag.repositories - ERROR - Traceback: Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 101, in map_httpcore_exceptions
    yield
  File "/app/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py", line 256, in handle_request
    raise exc from None
  File "/app/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py", line 103, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py", line 136, in handle_request
    raise exc
  File "/app/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py", line 106, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py", line 177, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py", line 217, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py", line 126, in read
    with map_exceptions(exc_map):
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/app/.venv/lib/python3.12/site-packages/httpcore/_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ReadTimeout: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/src/rag/repositories.py", line 217, in query
    response = query_engine.query(query_request.query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py", line 317, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/core/base/base_query_engine.py", line 44, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py", line 317, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 194, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py", line 317, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/base.py", line 235, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py", line 317, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/tree_summarize.py", line 159, in get_response
    response = self._llm.predict(
               ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py", line 317, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/core/llms/llm.py", line 623, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py", line 317, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/core/llms/callbacks.py", line 175, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/llms/ollama/base.py", line 343, in chat
    response = self.client.chat(
               ^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/ollama/_client.py", line 342, in chat
    return self._request(
           ^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/ollama/_client.py", line 180, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/ollama/_client.py", line 120, in _request_raw
    r = self._client.request(*args, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/httpx/_client.py", line 825, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/httpx/_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 249, in handle_request
    with map_httpcore_exceptions():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/app/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: timed out

2025-11-20 14:12:20,398 - src.rag.services - ERROR - Query failed for 'query='hi' top_k=2': timed out
2025-11-20 14:12:22,190 - src.history.repositories - INFO - Created query history with ID: 75c0198c-ed70-4e8c-9e65-85b1c85fe523
2025-11-20 14:12:22,212 - src.history.services - INFO - Saved query history with ID: 75c0198c-ed70-4e8c-9e65-85b1c85fe523
2025-11-20 14:12:22,344 - src.main - INFO - Response: 200 for POST /rag/query - 125.486s
2025-11-20 14:12:22,644 - src.main - INFO - Request: GET /history/queries from 172.67.182.229
2025-11-20 14:12:22,822 - src.main - INFO - Response: 200 for GET /history/queries - 0.181s
2025-11-20 14:15:36,396 - src.main - INFO - Request: POST /rag/query from 172.67.182.229
2025-11-20 14:15:38,227 - src.rag.repositories - INFO - Models configured: LLM=gemma3:4b, Embedding=embeddinggemma:latest
2025-11-20 14:15:45,653 - httpx - INFO - HTTP Request: POST http://ollama:11434/api/embed "HTTP/1.1 200 OK"
2025-11-20 14:15:45,677 - src.rag.repositories - INFO - Embedding dimension confirmed: 768
2025-11-20 14:15:45,718 - src.rag.repositories - INFO - Database connection established
2025-11-20 14:15:45,723 - src.rag.repositories - INFO - pgvector extension ensured
2025-11-20 14:15:45,912 - src.rag.repositories - INFO - Vector store configured with table 'documents' (embed_dim=768)
2025-11-20 14:15:45,946 - src.rag.repositories - INFO - Vector store contains 793 documents
2025-11-20 14:15:45,949 - src.rag.repositories - INFO - Index not initialized, creating from vector store...
2025-11-20 14:15:46,013 - src.rag.repositories - INFO - âœ“ Index successfully created from vector store
2025-11-20 14:15:46,014 - src.rag.repositories - INFO - Executing query: 'hi...'
2025-11-20 14:15:46,287 - httpx - INFO - HTTP Request: POST http://ollama:11434/api/show "HTTP/1.1 200 OK"
2025-11-20 14:15:46,644 - httpx - INFO - HTTP Request: POST http://ollama:11434/api/embed "HTTP/1.1 200 OK"
2025-11-20 14:15:50,553 - httpx - INFO - HTTP Request: POST http://ollama:11434/api/chat "HTTP/1.1 500 Internal Server Error"
2025-11-20 14:15:50,573 - src.rag.repositories - ERROR - Failed to execute query: model requires more system memory (7.4 GiB) than is available (5.2 GiB) (status code: 500)
2025-11-20 14:15:50,579 - src.rag.repositories - ERROR - Traceback: Traceback (most recent call last):
  File "/app/src/rag/repositories.py", line 217, in query
    response = query_engine.query(query_request.query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py", line 317, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/core/base/base_query_engine.py", line 44, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py", line 317, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 194, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py", line 317, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/base.py", line 235, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py", line 317, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/tree_summarize.py", line 159, in get_response
    response = self._llm.predict(
               ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py", line 317, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/core/llms/llm.py", line 623, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py", line 317, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/core/llms/callbacks.py", line 175, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/llms/ollama/base.py", line 343, in chat
    response = self.client.chat(
               ^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/ollama/_client.py", line 342, in chat
    return self._request(
           ^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/ollama/_client.py", line 180, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/ollama/_client.py", line 124, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: model requires more system memory (7.4 GiB) than is available (5.2 GiB) (status code: 500)

2025-11-20 14:15:50,581 - src.rag.services - ERROR - Query failed for 'query='hi' top_k=2': model requires more system memory (7.4 GiB) than is available (5.2 GiB) (status code: 500)
2025-11-20 14:15:50,614 - src.history.repositories - INFO - Created query history with ID: f8a4283c-442f-471c-9b2f-10d80d94bddc
2025-11-20 14:15:50,615 - src.history.services - INFO - Saved query history with ID: f8a4283c-442f-471c-9b2f-10d80d94bddc
2025-11-20 14:15:50,618 - src.main - INFO - Response: 200 for POST /rag/query - 14.222s
2025-11-20 14:15:50,654 - src.main - INFO - Request: GET /history/queries from 172.67.182.229
2025-11-20 14:15:50,703 - src.main - INFO - Response: 200 for GET /history/queries - 0.049s
2025-11-20 14:16:47,990 - src.main - INFO - Request: POST /rag/query from 172.67.182.229
2025-11-20 14:16:48,063 - src.rag.repositories - INFO - Models configured: LLM=gemma3:4b, Embedding=embeddinggemma:latest
2025-11-20 14:17:09,978 - httpx - INFO - HTTP Request: POST http://ollama:11434/api/embed "HTTP/1.1 200 OK"
2025-11-20 14:17:09,983 - src.rag.repositories - INFO - Embedding dimension confirmed: 768
2025-11-20 14:17:10,021 - src.rag.repositories - INFO - Database connection established
2025-11-20 14:17:10,024 - src.rag.repositories - INFO - pgvector extension ensured
2025-11-20 14:17:10,030 - src.rag.repositories - INFO - Vector store configured with table 'documents' (embed_dim=768)
2025-11-20 14:17:10,044 - src.rag.repositories - INFO - Vector store contains 793 documents
2025-11-20 14:17:10,044 - src.rag.repositories - INFO - Index not initialized, creating from vector store...
2025-11-20 14:17:10,047 - src.rag.repositories - INFO - âœ“ Index successfully created from vector store
2025-11-20 14:17:10,047 - src.rag.repositories - INFO - Executing query: 'What are principles this book talks?...'
2025-11-20 14:17:10,862 - httpx - INFO - HTTP Request: POST http://ollama:11434/api/show "HTTP/1.1 200 OK"
2025-11-20 14:17:11,982 - httpx - INFO - HTTP Request: POST http://ollama:11434/api/embed "HTTP/1.1 200 OK"
2025-11-20 14:17:14,603 - httpx - INFO - HTTP Request: POST http://ollama:11434/api/chat "HTTP/1.1 500 Internal Server Error"
2025-11-20 14:17:14,605 - src.rag.repositories - ERROR - Failed to execute query: model requires more system memory (7.4 GiB) than is available (5.9 GiB) (status code: 500)
2025-11-20 14:17:14,609 - src.rag.repositories - ERROR - Traceback: Traceback (most recent call last):
  File "/app/src/rag/repositories.py", line 217, in query
    response = query_engine.query(query_request.query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py", line 317, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/core/base/base_query_engine.py", line 44, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py", line 317, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 194, in _query
    response = self._response_synthesizer.synthesize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py", line 317, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/base.py", line 235, in synthesize
    response_str = self.get_response(
                   ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py", line 317, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/tree_summarize.py", line 159, in get_response
    response = self._llm.predict(
               ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py", line 317, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/core/llms/llm.py", line 623, in predict
    chat_response = self.chat(messages)
                    ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py", line 317, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/core/llms/callbacks.py", line 175, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/llama_index/llms/ollama/base.py", line 343, in chat
    response = self.client.chat(
               ^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/ollama/_client.py", line 342, in chat
    return self._request(
           ^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/ollama/_client.py", line 180, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/ollama/_client.py", line 124, in _request_raw
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: model requires more system memory (7.4 GiB) than is available (5.9 GiB) (status code: 500)

2025-11-20 14:17:14,610 - src.rag.services - ERROR - Query failed for 'query='What are principles this book talks?' top_k=2': model requires more system memory (7.4 GiB) than is available (5.9 GiB) (status code: 500)
2025-11-20 14:17:14,641 - src.history.repositories - INFO - Created query history with ID: 025ad52a-6375-4b16-9c9b-9750d15354b8
2025-11-20 14:17:14,642 - src.history.services - INFO - Saved query history with ID: 025ad52a-6375-4b16-9c9b-9750d15354b8
2025-11-20 14:17:14,643 - src.main - INFO - Response: 200 for POST /rag/query - 26.653s
2025-11-20 14:17:14,676 - src.main - INFO - Request: GET /history/queries from 172.67.182.229
2025-11-20 14:17:14,811 - src.main - INFO - Response: 200 for GET /history/queries - 0.135s
2025-11-20 14:17:41,178 - uvicorn.error - INFO - Shutting down
2025-11-20 14:17:41,301 - uvicorn.error - INFO - Waiting for application shutdown.
2025-11-20 14:17:41,322 - src.main - INFO - ðŸ›‘ Ultimate Advisor API server shutting down
2025-11-20 14:17:41,326 - uvicorn.error - INFO - Application shutdown complete.
2025-11-20 14:17:41,343 - uvicorn.error - INFO - Finished server process [10]
